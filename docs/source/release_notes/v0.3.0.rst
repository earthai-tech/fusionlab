.. _release_v0.3.0:

===============
Version 0.3.0
===============
*(Release Date: June 15, 2025)*

**Focus: Advanced PINNs and Flexible Attentive Architectures**

This is a milestone release that significantly refactors the core
attentive model architecture and introduces a powerful, flexible new
generation of Physics-Informed Neural Networks (PINNs). The focus has
been on modularity, robustness, and providing users with greater
control over the model architecture to tackle a wider range of
complex forecasting problems.

Enhancements & Improvements
~~~~~~~~~~~~~~~~~~~~~~~~~~~
* **New Flexible Base Architecture**:
    * |New| Introduced :class:`~fusionlab.nn.models.BaseAttentive`,
      a powerful base class that encapsulates the entire
      encoder-decoder and attention logic. This new class supports
      two distinct encoder architectures, selectable via the
      ``architecture`` parameter:
        * ``'hybrid'`` (default): Uses the :class:`~fusionlab.nn.components.MultiScaleLSTM`
          for sequence encoding.
        * ``'transformer'``: Uses a stack of pure self-attention
          layers for encoding, bypassing the LSTM.
    * |New| The base architecture now includes a ``mode`` parameter
      (``'tft_like'`` or ``'pihal_like'``) to control how known
      future features are handled, allowing for both TFT-style input
      enrichment and standard encoder-decoder data flows.

* **New Generation of Models**:
    * |New| **`TransFlowSubsNet`**: A new, state-of-the-art PINN for
      coupled geophysics, the
      :class:`~fusionlab.nn.pinn.TransFlowSubsNet`, is introduced.
      It inherits from `BaseAttentive` and is designed to model both
      groundwater flow and consolidation simultaneously.
    * |New| **`HALNet`**: The data-driven core,
      :class:`~fusionlab.nn.models.HALNet`, is now a standalone
      model inheriting from `BaseAttentive`. It provides a powerful,
      general-purpose forecasting tool without PINN components.
    * |New| **`PITGWFlow`**: A self-contained PINN for solving the
      2D transient groundwater flow equation using an MLP and a custom
      training step to minimize the PDE residual.

* **New and Improved Utilities**:
    * |New| Added :class:`~fusionlab.nn.forecast_tuner.HALTuner`, a
      dedicated tuner for the new ``HALNet`` model.
    * |Enhancement| Introduced a new visualization utility,
      :func:`~fusionlab.plot.forecast.plot_forecast_by_step`, which
      organizes plots by forecast step, making it easy to analyze
      model performance over the horizon. It gracefully falls back
      from spatial to temporal plots if coordinate data is missing.
    * |Enhancement| The :func:`~fusionlab.plot.forecast.forecast_view`
      and :func:`~fusionlab.plot.forecast.plot_forecasts` functions
      have been made more robust with improved color bar handling and
      auto-detection of value prefixes.
    * |Enhancement| The data preparation utility
      :func:`~fusionlab.nn.pinn.utils.prepare_pinn_data_sequences`
      is now mode-aware and can generate correctly shaped `future_features`
      for both `pihal_like` and `tft_like` architectures.

* **Code Example (New Flexible `HALNet`):**

  .. code-block:: python
     :linenos:

     import numpy as np
     import tensorflow as tf
     from fusionlab.nn.models import HALNet

     # 1. Define model config for "tft_like" mode
     TIME_STEPS, HORIZON = 8, 4
     halnet_tft = HALNet(
         mode='tft_like',
         static_input_dim=3, dynamic_input_dim=5, future_input_dim=2,
         output_dim=1, forecast_horizon=HORIZON, max_window_size=TIME_STEPS
     )

     # Data must span both lookback and forecast periods
     future_span_tft = TIME_STEPS + HORIZON
     inputs_tft = [
         tf.random.normal((2, 3)), # Static
         tf.random.normal((2, TIME_STEPS, 5)), # Dynamic
         tf.random.normal((2, future_span_tft, 2)) # Future
     ]
     output_tft = halnet_tft(inputs_tft)
     print(f"TFT-like mode output shape: {output_tft.shape}")

     # 2. Define model config for "pihal_like" mode
     halnet_pihal = HALNet(
         mode='pihal_like',
         static_input_dim=3, dynamic_input_dim=5, future_input_dim=2,
         output_dim=1, forecast_horizon=HORIZON, max_window_size=TIME_STEPS
     )
     # Data only needs to cover the forecast horizon for future features
     future_span_pihal = HORIZON
     inputs_pihal = [
         tf.random.normal((2, 3)), # Static
         tf.random.normal((2, TIME_STEPS, 5)), # Dynamic
         tf.random.normal((2, future_span_pihal, 2)) # Future
     ]
     output_pihal = halnet_pihal(inputs_pihal)
     print(f"PIHAL-like mode output shape: {output_pihal.shape}")

Fixes
~~~~~
* |Fix| **Architectural Overhaul**: Completely refactored the internal
  logic of `PIHALNet` (now `BaseAttentive`) to use a robust
  encoder-decoder architecture. This permanently fixes a series of
  `ValueError` and `InvalidArgumentError` exceptions related to
  shape mismatches that occurred when `time_steps` and
  `forecast_horizon` were different.
* |Fix| **Residual Connections**: Corrected the logic for residual
  connections (`Add` + `LayerNormalization`) to handle the
  `use_residuals=False` case correctly, preventing `TypeError`
  exceptions. All feature dimensions within the attention blocks are
  now consistent, resolving shape mismatches.
* |Fix| **Positional Encoding**: Replaced the naive linear positional
  encoding with the standard, robust sinusoidal implementation from
  "Attention Is All You Need". Fixed an issue where a single
  instance was incorrectly used on tensors with different feature
  dimensions.
* |Fix| **PINN Gradient Calculation**: Refactored `PITGWFlow` to
  decouple prediction from residual calculation. The `train_step` now
  manages a single `GradientTape` context, fixing a `ValueError` where
  gradients could not be computed due to a broken computational path.

Tests
~~~~~
* |Tests| Added a comprehensive Pytest suite for the new `HALNet`
  and `TransFlowSubsNet` models, validating both `'tft_like'` and
  `'pihal_like'` modes.
* |Tests| Created a robust test suite for `PITGWFlow`, covering
  instantiation, learnable parameter tracking, forward pass with
  multiple input formats, and the custom `train_step`.
* |Tests| Added a Pytest suite for the `PositionalEncoding` layer
  to ensure numerical stability, shape consistency, and serialization.

Documentation
~~~~~~~~~~~~~
* |Docs| Added a new User Guide page, :doc:`/user_guide/models/halnet`,
  to detail the flexible new `HALNet` model and its dual-mode
  architecture.
* |Docs| Updated the :doc:`/user_guide/pinn_models` page to reflect
  the new, more powerful `TransFlowSubsNet` and `PITGWFlow` models.
* |Docs| Added the :doc:`/user_guide/forecast_plots` page to
  document the new and improved visualization utilities, including
  ``forecast_view``, ``plot_forecast_by_step``, and ``plot_history_in``.
* |Docs| Added an exercise page, :doc:`/user_guide/exercices/exercise_halnet`,
  to provide a hands-on tutorial for using the new `HALNet` model.

Contributors
~~~~~~~~~~~~~
* `Laurent Kouadio <https://earthai-tech.github.io/>`_ (Lead Developer)
