GWFlowPDEResidual (Keras Layer)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
:API Reference: :class:`~fusionlab.nn.pinn.base.GWFlowPDEResidual`

The ``GroundwaterFlowPDEResidual`` is a ``tf.keras.layers.Layer`` specifically
designed to compute the residual of the 2D groundwater flow equation. It
leverages automatic differentiation (via ``tf.GradientTape``) to calculate
the necessary partial derivatives of the predicted hydraulic head (:math:`h`).

The standard 2D isotropic groundwater flow equation is:

.. math::
    S_s \frac{\partial h}{\partial t} - K \left( \frac{\partial^2 h}{\partial x^2} + \frac{\partial^2 h}{\partial y^2} \right) - Q = \mathcal{R}

where :math:`\mathcal{R}` is the residual that the PINN aims to minimize.

**Key Features:**

* **Keras Layer:** Integrates seamlessly into Keras model architectures.
* **Automatic Differentiation:** Computes first and second-order partial
    derivatives of the input hydraulic head prediction (:math:`h_{pred}`)
    with respect to time (:math:`t`) and spatial coordinates (:math:`x, y`).
* **Coefficient Management:** Internally uses an instance of
    :class:`~fusionlab.nn.pinn.base.GWResidualCalculator` to manage the
    physical coefficients :math:`S_s`, :math:`K`, and :math:`Q`. These
    coefficients can be learnable or fixed, as configured through the
    ``gw_flow_coeffs_config`` parameter.
* **Input Structure:** The ``call`` method expects a tuple of four tensors:
    `(h_pred, t_coords, x_coords, y_coords)`. It's crucial that `h_pred`
    is differentiable with respect to `t_coords`, `x_coords`, and `y_coords`
    within the TensorFlow graph.

**When to Use:**

Incorporate this layer into your PINN model when you need to enforce the
2D groundwater flow equation as a physics-based constraint:

* As part of your model's ``call`` method, where you compute predictions
    and then calculate the PDE residual using these predictions and their
    corresponding coordinates.
* The output of this layer (the PDE residual) would then be used in the
    physics part of your composite loss function during training.

**Code Example (Usage within a conceptual model):**

.. code-block:: python
   :linenos:

   import tensorflow as tf
   from fusionlab.nn.pinn.base import GWFlowPDEResidual

   # Assume `head_prediction_network` is a Keras model that takes
   # (t,x,y) coordinates (or features derived from them) and outputs h_pred.
   # class HeadPredictor(tf.keras.Model):
   # ... (definition of a model that outputs h_pred) ...
   # head_predictor = HeadPredictor()

   # Define configuration for the groundwater flow coefficients
   gw_coeffs_config = {
       'K': 'learnable',  # Hydraulic conductivity
       'Ss': 1e-5,        # Specific storage (fixed)
       'Q': 0.0           # Source/sink term (fixed at zero)
   }

   # Instantiate the PDE residual layer
   gw_pde_layer = GWFlowPDEResidual(
       gw_flow_coeffs_config=gw_coeffs_config,
       name="gw_pde_loss_calculator"
   )

   # Example dummy inputs for a PINN
   batch_size = 4
   num_collocation_points = 100 # Points where PDE is evaluated
   dummy_t = tf.random.uniform((batch_size, num_collocation_points, 1), dtype=tf.float32)
   dummy_x = tf.random.uniform((batch_size, num_collocation_points, 1), dtype=tf.float32)
   dummy_y = tf.random.uniform((batch_size, num_collocation_points, 1), dtype=tf.float32)

   # In a real PINN, h_pred would come from your neural network,
   # and it must be differentiable with respect to t, x, y.
   # For this example, let's simulate h_pred using a simple function of t,x,y
   # so that gradients are non-zero.
   
   # This part simulates how PIHALNet might generate h_pred (e.g., gwl_pred_mean)
   # using a sub-network, ensuring it's part of the gradient tape.
   # We need a tape here to ensure h_pred is differentiable when passed to gw_pde_layer
   # (though gw_pde_layer uses its own internal tapes for the actual PDE derivatives).

   class MySimpleNet(tf.keras.Model): # A simple model for demonstration
       def __init__(self):
           super().__init__()
           self.dense = tf.keras.layers.Dense(1)
       def call(self, t, x, y):
           coords_combined = tf.concat([t, x, y], axis=-1)
           return self.dense(coords_combined)

   simple_h_net = MySimpleNet()

   with tf.GradientTape(watch_accessed_variables=False) as outer_tape:
       outer_tape.watch(dummy_t)
       outer_tape.watch(dummy_x)
       outer_tape.watch(dummy_y)
       # Simulate h_pred being generated by a network that takes t,x,y
       # This is crucial for AD to work inside the gw_pde_layer
       simulated_h_pred = simple_h_net(dummy_t, dummy_x, dummy_y)

   # Now, call the PDE residual layer
   # Inputs: (h_pred, t_coords, x_coords, y_coords)
   pde_residuals = gw_pde_layer((simulated_h_pred, dummy_t, dummy_x, dummy_y))

   # print(f"Shape of simulated_h_pred: {simulated_h_pred.shape}")
   # print(f"Shape of PDE residuals: {pde_residuals.shape}")
   # print(f"Example PDE residual values: {pde_residuals[0, :3, 0].numpy()}")

   # Trainable variables from the PDE layer (includes learnable K)
   # print("Trainable variables in gw_pde_layer:", [v.name for v in gw_pde_layer.trainable_variables])

.. raw:: html

   <hr style="margin-top: 1.5em; margin-bottom: 1.5em;">